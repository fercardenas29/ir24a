{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab838f6833b5dfdb",
   "metadata": {},
   "source": [
    "# Integrating Embeddings with Queries in an Information Retrieval System\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this exercise, we will learn how to integrate embeddings with a query to enhance an Information Retrieval (IR) system. We will use both static and contextual embeddings to generate representations of queries and documents, compute their similarities, and rank the documents based on relevance to the query.\n",
    "\n",
    "---\n",
    "\n",
    "## Stages Covered\n",
    "\n",
    "1. **Introduction to Pre-trained Transformer Models**\n",
    "   - Load and use BERT for contextual embeddings.\n",
    "   - Load and use Word2Vec for static embeddings.\n",
    "\n",
    "2. **Generating Text Embeddings**\n",
    "   - Generate embeddings for queries and documents using BERT.\n",
    "   - Generate embeddings for queries and documents using Word2Vec.\n",
    "\n",
    "3. **Computing Similarity Between Embeddings**\n",
    "   - Compute cosine similarity between query and document embeddings.\n",
    "   - Rank documents based on similarity scores.\n",
    "\n",
    "4. **Integrating Embeddings with Queries**\n",
    "   - Practical implementation of embedding-based retrieval for a given text corpus.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- TensorFlow\n",
    "- Hugging Face's Transformers library\n",
    "- Gensim library\n",
    "- Scikit-learn library\n",
    "- A text corpus in the `../data` folder\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Follow the steps below to integrate embeddings with a query and enhance your IR system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96332a5cf057238",
   "metadata": {},
   "source": [
    "Step 0: Verify requirements:\n",
    "\n",
    "* tensorflow\n",
    "* transformers\n",
    "* scikit-learn\n",
    "* matplotlib\n",
    "* seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f333a9f10d93ad",
   "metadata": {},
   "source": [
    "Step 1: Download dataset from Kaggle\n",
    "\n",
    "URL: https://www.kaggle.com/datasets/zynicide/wine-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bab6504ab1e044a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T21:05:14.438938Z",
     "start_time": "2024-06-26T21:05:12.777794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0 country                                        description  \\\n",
      "0           0      US  This tremendous 100% varietal wine hails from ...   \n",
      "1           1   Spain  Ripe aromas of fig, blackberry and cassis are ...   \n",
      "2           2      US  Mac Watson honors the memory of a wine once ma...   \n",
      "3           3      US  This spent 20 months in 30% new French oak, an...   \n",
      "4           4  France  This is the top wine from La Bégude, named aft...   \n",
      "\n",
      "                            designation  points  price        province  \\\n",
      "0                     Martha's Vineyard      96  235.0      California   \n",
      "1  Carodorum Selección Especial Reserva      96  110.0  Northern Spain   \n",
      "2         Special Selected Late Harvest      96   90.0      California   \n",
      "3                               Reserve      96   65.0          Oregon   \n",
      "4                            La Brûlade      95   66.0        Provence   \n",
      "\n",
      "            region_1           region_2             variety  \\\n",
      "0        Napa Valley               Napa  Cabernet Sauvignon   \n",
      "1               Toro                NaN       Tinta de Toro   \n",
      "2     Knights Valley             Sonoma     Sauvignon Blanc   \n",
      "3  Willamette Valley  Willamette Valley          Pinot Noir   \n",
      "4             Bandol                NaN  Provence red blend   \n",
      "\n",
      "                    winery  \n",
      "0                    Heitz  \n",
      "1  Bodega Carmen Rodríguez  \n",
      "2                 Macauley  \n",
      "3                    Ponzi  \n",
      "4     Domaine de la Bégude  \n"
     ]
    }
   ],
   "source": [
    "#import kaggle\n",
    "import pandas as pd\n",
    "#kaggle.api.dataset_download_cli(dataset='zynicide/wine-reviews')\n",
    "\n",
    "wine_df = pd.read_csv('data/winemag-data_first150k.csv')\n",
    "#print(wine_df.head())\n",
    "corpus = wine_df[:50]\n",
    "print(corpus.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef79a9c697bc01",
   "metadata": {},
   "source": [
    "Step 2: Load a Pre-trained Transformer Model\n",
    "\n",
    "Use the BERT model for generating contextual embeddings and Word2Vec for static embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6936023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Ruta al archivo descargado\n",
    "model_path = 'data/GoogleNews-vectors-negative300.bin.gz'\n",
    "# Cargar el modelo Word2Vec preentrenado\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68840411ddabd35a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T21:06:28.907164Z",
     "start_time": "2024-06-26T21:05:21.570520Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\usuario\\Fer-Pc\\Escritorio\\EPN\\2024-A\\SEPTIMO_SEMESTRE\\RECUPERACION_DE_INFORMACION\\ir24a\\venv3.8\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92ce2c32618191",
   "metadata": {},
   "source": [
    "Step 3: Generate Text Embeddings\n",
    "\n",
    "Static Embeddings with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53983765626d5a85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T21:07:16.011459Z",
     "start_time": "2024-06-26T21:07:15.967338Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_word2vec_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        tokens = text.lower().split()\n",
    "        word_vectors = [word2vec_model[word] for word in tokens if word in word2vec_model]\n",
    "        if word_vectors:\n",
    "            embeddings.append(np.mean(word_vectors, axis=0))\n",
    "        else:\n",
    "            embeddings.append(np.zeros(word2vec_model.vector_size))\n",
    "    return np.array(embeddings)\n",
    "\n",
    "word2vec_embeddings = generate_word2vec_embeddings(wine_df[:10])\n",
    "#print(\"Word2Vec Embeddings:\", word2vec_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2eee6a2cae63f9",
   "metadata": {},
   "source": [
    "Contextual Embeddings with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deddc992f933fcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T02:43:37.685104Z",
     "start_time": "2024-06-27T02:43:33.304379Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_bert_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True)\n",
    "        outputs = bert_model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state[:, 0, :])  # Use [CLS] token representation\n",
    "    return np.array(embeddings).transpose(0,2,1)\n",
    "\n",
    "bert_embeddings = generate_bert_embeddings(wine_df[:10])\n",
    "#print(\"BERT Embeddings:\", bert_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f905d2f5fe852",
   "metadata": {},
   "source": [
    "Step 4: Compute Similarity Between Embeddings\n",
    "\n",
    "Use the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1c1a78ceff1d324",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T02:46:20.622551Z",
     "start_time": "2024-06-27T02:46:20.606815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.metrics.pairwise import cosine_similarity\\n\\n# Cosine similarity between Word2Vec embeddings\\nword2vec_similarity = cosine_similarity(word2vec_embeddings)\\nprint(\"Word2Vec Cosine Similarity:\\n\", word2vec_similarity)\\n\\n# Cosine similarity between BERT embeddings\\nbert_similarity = cosine_similarity(bert_embeddings.reshape(10,768))\\nprint(\"BERT Cosine Similarity:\\n\", bert_similarity)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Cosine similarity between Word2Vec embeddings\n",
    "word2vec_similarity = cosine_similarity(word2vec_embeddings)\n",
    "print(\"Word2Vec Cosine Similarity:\\n\", word2vec_similarity)\n",
    "\n",
    "# Cosine similarity between BERT embeddings\n",
    "bert_similarity = cosine_similarity(bert_embeddings.reshape(10,768))\n",
    "print(\"BERT Cosine Similarity:\\n\", bert_similarity)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46c4c975cb5c7b",
   "metadata": {},
   "source": [
    "Step 5: Compare Contextual and Static Embeddings\n",
    "\n",
    "Analyze and compare the similarity results from both BERT and Word2Vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7416e3d9334cd21e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T03:25:28.401064Z",
     "start_time": "2024-06-27T03:25:27.726620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\ndef plot_similarity_matrix(matrix, title, figsize=(8, 6), annotation=True):\\n    plt.figure(figsize=figsize)\\n    sns.heatmap(matrix, annot=annotation, cmap=\\'coolwarm\\', fmt=\\'.2f\\')\\n    plt.title(title)\\n    plt.show()\\n\\nplot_similarity_matrix(word2vec_similarity, \"Word2Vec Cosine Similarity\")\\nplot_similarity_matrix(bert_similarity, \"BERT Cosine Similarity\")\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_similarity_matrix(matrix, title, figsize=(8, 6), annotation=True):\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(matrix, annot=annotation, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_similarity_matrix(word2vec_similarity, \"Word2Vec Cosine Similarity\")\n",
    "plot_similarity_matrix(bert_similarity, \"BERT Cosine Similarity\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1bd3d58f27a375",
   "metadata": {},
   "source": [
    "Step 6: Applying to Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5696a6b93f33889b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T03:27:06.030478Z",
     "start_time": "2024-06-27T03:25:33.511959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Generate embeddings for the corpus\\ncorpus_word2vec_embeddings = generate_word2vec_embeddings(corpus[:500])\\ncorpus_bert_embeddings = generate_bert_embeddings(corpus[:500])\\n\\n# Compute similarity for the corpus\\ncorpus_word2vec_similarity = cosine_similarity(corpus_word2vec_embeddings)\\ncorpus_bert_similarity = cosine_similarity(corpus_bert_embeddings.reshape(corpus_bert_embeddings.shape[:2]))\\n\\n# Plot similarity matrices\\nplot_similarity_matrix(corpus_word2vec_similarity, \"Corpus Word2Vec Cosine Similarity\", figsize=(16, 12), annotation=False)\\nplot_similarity_matrix(corpus_bert_similarity, \"Corpus BERT Cosine Similarity\", figsize=(16, 12), annotation=False)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Generate embeddings for the corpus\n",
    "corpus_word2vec_embeddings = generate_word2vec_embeddings(corpus[:500])\n",
    "corpus_bert_embeddings = generate_bert_embeddings(corpus[:500])\n",
    "\n",
    "# Compute similarity for the corpus\n",
    "corpus_word2vec_similarity = cosine_similarity(corpus_word2vec_embeddings)\n",
    "corpus_bert_similarity = cosine_similarity(corpus_bert_embeddings.reshape(corpus_bert_embeddings.shape[:2]))\n",
    "\n",
    "# Plot similarity matrices\n",
    "plot_similarity_matrix(corpus_word2vec_similarity, \"Corpus Word2Vec Cosine Similarity\", figsize=(16, 12), annotation=False)\n",
    "plot_similarity_matrix(corpus_bert_similarity, \"Corpus BERT Cosine Similarity\", figsize=(16, 12), annotation=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b051daec",
   "metadata": {},
   "source": [
    "### Programacion Parelela\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "271c47f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "#Funcion paralela para word2vec\n",
    "def generate_word2vec_embeddings_parallel(text):\n",
    "    num_nucleos = multiprocessing.cpu_count()\n",
    "    pool = multiprocessing.Pool(processes=num_nucleos)\n",
    "\n",
    "    embeddings = pool.map(generate_word2vec_embeddings, text)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02893943",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_word2Vec = generate_word2vec_embeddings_parallel(wine_df[:1])\n",
    "print(\"Word Embeddings:\", corpus_word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be33d0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "#Funcion paralela para bert\n",
    "def generate_bert_embeddings_parallel(texts):\n",
    "    num_nucleos = multiprocessing.cpu_count()\n",
    "    pool = multiprocessing.Pool(processes=num_nucleos)\n",
    "\n",
    "    embeddings = pool.map(generate_bert_embeddings, texts)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec32d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bert = generate_bert_embeddings_parallel(wine_df[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bac5300222c91be",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "So far, in this exercise, you learned how to:\n",
    "\n",
    "* Load a pre-trained transformer model (BERT) and a static embedding model (Word2Vec).\n",
    "* Generate text embeddings using these models.\n",
    "* Compute cosine similarity between embeddings.\n",
    "* Compare the similarity results from both contextual and static embeddings.\n",
    "\n",
    "Now you have a practical understanding of how transformers and embeddings can be used in Information Retrieval systems.\n",
    "\n",
    "Let's integrate query search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc424507979ba874",
   "metadata": {},
   "source": [
    "Step 7: Generate Embeddings for the Query\n",
    "\n",
    "Generate embeddings for the query using the same model used for the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "658207660a566a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['computer science']\n",
    "#query_word2vec_embeddings = generate_word2vec_embeddings([query])\n",
    "query_bert_embeddings = generate_bert_embeddings([query])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389cb8f527843221",
   "metadata": {},
   "source": [
    "Step 8: Compute Similarity Between Query and Documents\n",
    "\n",
    "Compute the similarity between the query embedding and each document embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5991a0aeababffc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. check_pairwise_arrays expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Calcular la similitud coseno entre la consulta y los embeddings del corpus\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m cos_similarities \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bert_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_embeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\usuario\\Fer-Pc\\Escritorio\\EPN\\2024-A\\SEPTIMO_SEMESTRE\\RECUPERACION_DE_INFORMACION\\ir24a\\venv3.8\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\usuario\\Fer-Pc\\Escritorio\\EPN\\2024-A\\SEPTIMO_SEMESTRE\\RECUPERACION_DE_INFORMACION\\ir24a\\venv3.8\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1578\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1544\u001b[0m \n\u001b[0;32m   1545\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1574\u001b[0m \u001b[38;5;124;03m    Returns the cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1578\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1580\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mc:\\Users\\usuario\\Fer-Pc\\Escritorio\\EPN\\2024-A\\SEPTIMO_SEMESTRE\\RECUPERACION_DE_INFORMACION\\ir24a\\venv3.8\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:165\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    156\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    157\u001b[0m         X,\n\u001b[0;32m    158\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m     Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    174\u001b[0m         Y,\n\u001b[0;32m    175\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    179\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    180\u001b[0m     )\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n",
      "File \u001b[1;32mc:\\Users\\usuario\\Fer-Pc\\Escritorio\\EPN\\2024-A\\SEPTIMO_SEMESTRE\\RECUPERACION_DE_INFORMACION\\ir24a\\venv3.8\\lib\\site-packages\\sklearn\\utils\\validation.py:951\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    949\u001b[0m     )\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    952\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    954\u001b[0m     )\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m    957\u001b[0m     _assert_all_finite(\n\u001b[0;32m    958\u001b[0m         array,\n\u001b[0;32m    959\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    960\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    961\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    962\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. check_pairwise_arrays expected <= 2."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Calcular la similitud coseno entre la consulta y los embeddings del corpus\n",
    "cos_similarities = cosine_similarity(query_bert_embeddings, bert_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139fea5d43934c96",
   "metadata": {},
   "source": [
    "Step 9: Retrieve and Rank Documents Based on Similarity Scores\n",
    "\n",
    "Retrieve and rank the documents based on their similarity scores to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a6b13bbb20d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7721ececc688c1be",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
