{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Web Scraping Exercise\n",
    "\n",
    "## 1. Introduction and Planning\n",
    "\n",
    "### Objective:\n",
    "The goal of this exercise is to build a web scraper that collects data from a chosen website. You will learn how to send HTTP requests, parse HTML content, extract relevant data, and store it in a structured format.\n",
    "\n",
    "### Tasks:\n",
    "1. Identify the data you want to scrape.\n",
    "2. Choose the target website(s).\n",
    "3. Plan the structure of your project.\n",
    "\n",
    "### Example:\n",
    "For this exercise, we will scrape job listings from Indeed.com. We will extract job titles, company names, locations, and job descriptions."
   ],
   "id": "88b63181c563a666"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Understanding the Target Website\n",
    "### Objective:\n",
    "\n",
    "Analyze the structure of the web pages to be scraped.\n",
    "### Tasks:\n",
    "\n",
    "* Inspect the target website using browser developer tools.\n",
    "* Identify the HTML elements that contain the desired data.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "* Open your browser and navigate to the target website (e.g., Indeed.com).\n",
    "* Right-click on the webpage and select \"Inspect\" or press Ctrl+Shift+I.\n",
    "* Use the developer tools to explore the HTML structure of the webpage.\n",
    "* Identify the tags and classes of the elements that contain the job titles, company names, locations, and descriptions."
   ],
   "id": "477cce632174e459"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Writing the Scraper\n",
    "### Objective:\n",
    "\n",
    "Develop the code to scrape data from the target website.\n",
    "### Tasks:\n",
    "\n",
    "* Send HTTP requests to the target website.\n",
    "* Parse the HTML content and extract the required data.\n",
    "* Handle pagination to scrape data from multiple pages.\n",
    "* Implement error handling."
   ],
   "id": "393d4bc45393e6b5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL of the target website\n",
    "url = 'https://www.indeed.com/jobs?q=data+scientist&l='\n",
    "\n",
    "# Define a user-agent to make the request look like it's coming from a web browser\n",
    "headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Cache-Control\": \"max-age=0\",\n",
    "    }\n",
    "# Send an HTTP request to the target URL\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract the job listings\n",
    "    job_titles = soup.find_all('h2', class_='jobTitle')\n",
    "    companies = soup.find_all('span', class_='companyName')\n",
    "    locations = soup.find_all('div', class_='companyLocation')\n",
    "    descriptions = soup.find_all('div', class_='job-snippet')\n",
    "    \n",
    "    # Print the extracted data\n",
    "    for title, company, location, description in zip(job_titles, companies, locations, descriptions):\n",
    "        print(f'Job Title: {title.get_text(strip=True)}')\n",
    "        print(f'Company: {company.get_text(strip=True)}')\n",
    "        print(f'Location: {location.get_text(strip=True)}')\n",
    "        print(f'Description: {description.get_text(strip=True)}')\n",
    "        print('-' * 40)\n",
    "else:\n",
    "    print(f'Failed to retrieve the webpage. Status code: {response.status_code}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "574f5c62340afb7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6ff760d1b7d059d1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
